{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### üîç Objective:\nThis project will **introduce you to CNNs**.\n\n### üìå What You‚Äôll Do:\n1. Define suitable transforms/augmentations for your `train` and `test` images.\n2. Pass these images into PyTorch `DataLoaders` for batch processing.\n3. Implement `CNN` class architecture for pneumonia image classification.\n4. Train and validate your model.\n\nüí° **PLEASE PLEASE PLEASE look things up!!! This is YOUR learning experience.**\n\n---","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ntrain_path = '/kaggle/input/chest-xray-pneumonia/chest_xray/train/'\ntest_path = '/kaggle/input/chest-xray-pneumonia/chest_xray/test/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T23:24:55.964380Z","iopub.execute_input":"2025-01-30T23:24:55.964653Z","iopub.status.idle":"2025-01-30T23:24:59.878536Z","shell.execute_reply.started":"2025-01-30T23:24:55.964628Z","shell.execute_reply":"2025-01-30T23:24:59.877445Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"#### üìå ***TASK 1 - DATA PREPROCESSING*** \n\nDefine image augmentations in the cell below using two variables:  \n\n- **`transform_train`**: Stores transforms for training images. You can include any augmentations you prefer.  \n- **`transform_test`**: Stores transforms for your test images. As a best practice, limit these transformations to only the essential ones from `transform_train`.\n\nLastly, be sure to convert all images to [tensors](https://www.perplexity.ai/search/i-m-a-student-at-naiss-mlb-and-_EL_nBO9TS694cbTEl5M.A) via the `transforms.ToTensor()` transform. Don't know transforms? [Click here](https://pytorch.org/vision/stable/transforms.html). ","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\n\n...","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we'll pass our images and transforms into a `DataLoader`, which allows us to train our model in batches.\nMost of the code is done for you, but [click here](https://www.perplexity.ai/search/i-m-a-student-at-naiss-mlb-and-_EL_nBO9TS694cbTEl5M.A) to learn more.","metadata":{}},{"cell_type":"code","source":"from torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\n\nnum_batches = 32  # Feel free to change this\n\ntrain_dataset = ImageFolder(root=train_path, transform=...) # Your Transformed train images\ntrain_loader = DataLoader(dataset=..., batch_size=num_batches, shuffle=True) \n\ntest_dataset = ImageFolder(root=test_path, transform=...) # Your transformed test images\ntest_loader = DataLoader(dataset=..., batch_size=num_batches, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### üìå ***TASK 2 - CNN Architecture***\n\nThis is where you have all the creative freedom in the world. Here are some good questions to ask yourself: \n\n- How many [channels](https://www.perplexity.ai/search/i-m-a-student-at-naiss-mlb-wha-49AG77e4Qp2e7EkARdFsTA) should go into the input layer?\n- What measures can I take to avoid [overfitting](https://www.perplexity.ai/search/i-m-a-student-at-naiss-mlb-wha-YdAbhqQzRZaEq39BEQzA6w)?\n- What matters to me? (Training Speed / Performance tradeoffs)\n- **CONVOLUTION. ACTIVATION FUNCTION. POOLING!!!** üì¢üì¢üì¢\n\nNot comfortable with PyTorch? [Here](https://youtu.be/mozBidd58VQ?si=TE2_81TEQko1eDXT). Go and make me the best [CNN](https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns) I've ever seen :) ","metadata":{}},{"cell_type":"code","source":"import torch \nimport torch.nn \n\n# Remember what Pooling does to feature maps!!!\nimage_width = ...\nimage_height = ... \n\n...","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### üìå ***TASK 3 - DEFINE TRAIN FUNCTION***  \n\nDefine `process_forward_phase` and `train` to update model weights with each new [epoch/iteration](https://www.perplexity.ai/search/i-m-a-curious-naiss-mlb-studen-7SJECNYrS1iYxUR032dp7A). Here are the steps: \n\n- **Forward pass:** Here, our batch is taken through the network to output a prediction (Normal/Pneumonia)\n- **Backward pass:** The model goes \"What's our loss? Hmmm... Not quite what I want. This means my `weights` aren't adjusted properly. Let me propagate my `loss` backward in hopes of correcting my weights.\"\n\nWe use **`f1_score`** as the primary metric and also display **`accuracy`** for comparison. Most steps are outlined for you‚Äîjust follow the structure provided!","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm # Visualize training progress \nfrom sklearn.metrics import f1_score, accuracy_score\n\ndef process_forward_pass(model, batch, criterion):\n    \"\"\"\n    This is a helper function to abstract the \"forward\"\n    phase of the training loop. This function also returns \n    the loss, predictions, and labels seen in the batch. \n    \"\"\"\n    \n    images, labels = batch\n    labels = labels.float()\n\n    ... \n    \n    return loss, preds, labels \n\ndef train(model, train_loader, criterion, optimizer, epochs):\n    ... # Set model to training mode\n    \n    for epoch in range(...): # Specify the number of iterations to train for\n        all_preds, all_labels = [], []\n\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n            optimizer.zero_grad()\n            loss, preds, labels = process_forward_phase(model, batch, criterion)\n\n            ... # (backward phase, upd. weights)\n\n            all_preds.extend(preds.numpy())\n            all_labels.extend(labels.numpy())\n\n        accuracy = accuracy_score(all_labels, all_preds)\n        f1 = f1_score(all_labels, all_preds)\n\n        print(f\"Acc={accuracy:.2f}%, F1={f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T14:38:35.587676Z","iopub.execute_input":"2025-02-07T14:38:35.588043Z","iopub.status.idle":"2025-02-07T14:38:36.250754Z","shell.execute_reply.started":"2025-02-07T14:38:35.588017Z","shell.execute_reply":"2025-02-07T14:38:36.249692Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"After your model trains, you want to see how well it performs on **unseen data.** Meaning, if this were a live hospital NEEDING your predictions to classify patients with pneumonia, how well would it do? \n\n\nYou simply have to run this cell; all the code is implemented for you (Assuming `process_forward_phase` works fine). üòä","metadata":{}},{"cell_type":"code","source":"def test(model, test_loader, criterion):\n    model.eval()\n    all_preds, all_labels = [], []\n    \n    with torch.no_grad():\n        for batch in test_loader: \n            loss, preds, labels = process_forward_phase(model, batch, criterion)\n            all_preds.extend(preds.numpy())\n            all_labels.extend(labels.numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds)\n\n    print(f\"Final Test Results: Acc={accuracy:.2f}%, F1={f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T13:46:23.454693Z","iopub.execute_input":"2025-02-07T13:46:23.455211Z","iopub.status.idle":"2025-02-07T13:46:23.464758Z","shell.execute_reply.started":"2025-02-07T13:46:23.455043Z","shell.execute_reply":"2025-02-07T13:46:23.463432Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"#### üìå***TASK 4 - TRAIN MODEL***\n\nWe're close!!! We simply need to instantiate the `model`, define a suitable `criterion` (loss), and use an `optimizer` (thing to speed up backpropagation).","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\n... \noptimizer = optim.Adam(model.parameters(), lr=..., weight_decay=...) # Weight decay is optional. But is it? ü§î\n\ntrain(model, train_loader, criterion, optimizer, epochs=...)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Last step: evaluate your model's performance. Remember, you get **1,000,000** brownie points üç´ if you beat Adam's **`f1_score:`0.8549**. ","metadata":{}},{"cell_type":"code","source":"test(model, test_loader, criterion)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}